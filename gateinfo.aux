\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{ntk,arora2019exact,cao2019generalization,dudnn}
\citation{arora2019exact}
\citation{ntk,dudnn,arora2019exact,cao2019generalization}
\citation{arora2019exact}
\citation{arora2019exact}
\citation{cao2019generalization}
\citation{arora2019exact}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Background: Neural Tangent Feature and Kernel}{2}{subsection.1.1}}
\newlabel{sec:background}{{1.1}{2}{Background: Neural Tangent Feature and Kernel}{subsection.1.1}{}}
\newlabel{sec:background@cref}{{[subsection][1][1]1.1}{[1][2][]2}}
\newlabel{prop:basic}{{1.1}{2}{\textbf {Lemma 3.1} \cite {arora2019exact}}{proposition.1.1}{}}
\newlabel{prop:basic@cref}{{[proposition][1][1]1.1}{[1][2][]2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Our Contributions}{2}{subsection.1.2}}
\newlabel{sec:contrib}{{1.2}{2}{Our Contributions}{subsection.1.2}{}}
\newlabel{sec:contrib@cref}{{[subsection][2][1]1.2}{[1][2][]2}}
\citation{ntk,arora2019exact,cao2019generalization}
\citation{arora2019exact}
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Path Feature and Kernel: Encoding Gating Information}{3}{section.2}}
\newlabel{sec:path}{{2}{3}{Neural Path Feature and Kernel: Encoding Gating Information}{section.2}{}}
\newlabel{sec:path@cref}{{[section][2][]2}{[1][3][]3}}
\newlabel{eq:npfnpv}{{1}{3}{Neural Path Feature and Kernel: Encoding Gating Information}{equation.2.1}{}}
\newlabel{eq:npfnpv@cref}{{[equation][1][]1}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Paths}{3}{subsection.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces DNN with ReLU activation. Here, $x\in \mathbb  {R}^{d_{in}}$ is the input to the DNN, and $\mathaccentV {hat}05E{y}_{\Theta }(x)$ is the output, `$q$'s are pre-activation inputs, `$z$'s are output of the hidden layers, `$G$'s are the gating values. $l\in [d-1]$ is the index of the layer, and $i\in [w]$ is the index of the hidden units in a layer.\relax }}{4}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tb:basic}{{1}{4}{DNN with ReLU activation. Here, $x\in \R ^{d_{in}}$ is the input to the DNN, and $\hat {y}_{\Theta }(x)$ is the output, `$q$'s are pre-activation inputs, `$z$'s are output of the hidden layers, `$G$'s are the gating values. $l\in [d-1]$ is the index of the layer, and $i\in [w]$ is the index of the hidden units in a layer.\relax }{table.caption.2}{}}
\newlabel{tb:basic@cref}{{[table][1][]1}{[1][3][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neural Path Feature, Neural Path Value and Network Output}{4}{subsection.2.2}}
\newlabel{def:nps}{{2.1}{4}{}{definition.2.1}{}}
\newlabel{def:nps@cref}{{[definition][1][2]2.1}{[1][3][]4}}
\newlabel{prop:zero}{{2.1}{4}{}{proposition.2.1}{}}
\newlabel{prop:zero@cref}{{[proposition][1][2]2.1}{[1][4][]4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A toy illustration of gates, paths and active sub-networks. The cartoon (\textbf  {a}) in the top left corner shows a DNN with $2$ hidden layers, $6$ ReLU gates $G(l,i),l=1,2,i=1,2,3$, $2$ input nodes $x(1)$ and $x(2)$ and an output node $\mathaccentV {hat}05E{y}_{\Theta }(x)$. Cartoons (\textbf  {b}) to (\textbf  {g}) show the enumeration of the paths $p_1,\ldots  , p_{18}$. Cartoons (\textbf  {h}), (\textbf  {i}) and (\textbf  {j}) show hypothetical gates for $3$ different hypothetical input examples $\{x_s\}_{s=1}^3 \in \mathbb  {R}^2$. In each of the cartoons (\textbf  {h}), (\textbf  {i}) and (\textbf  {j}), the $1/0$ inside the circles denotes the on/off state of the gates, and the bold paths/gates shown in red colour constitute the active sub-network for that particular input example. The NPFs are given by $\phi _{x}=[x(1)A(x,p_1),\ldots  ,x(1)A(x,p_{9}),x(2)A(x,p_{10}),\ldots  ,x(2)A(x,p_{18})]^\top $.\relax }}{4}{figure.caption.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Neural Path Kernel : Similarity based on active sub-networks}{4}{subsection.2.3}}
\newlabel{def:lambda}{{2.2}{4}{}{definition.2.2}{}}
\newlabel{def:lambda@cref}{{[definition][2][2]2.2}{[1][4][]4}}
\citation{shamir,dudln}
\newlabel{lm:npk}{{2.1}{5}{}{lemma.2.1}{}}
\newlabel{lm:npk@cref}{{[lemma][1][2]2.1}{[1][4][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Dynamics of Gradient Descent with NPF and NPV Learning}{5}{section.3}}
\newlabel{sec:gatedyna}{{3}{5}{Dynamics of Gradient Descent with NPF and NPV Learning}{section.3}{}}
\newlabel{sec:gatedyna@cref}{{[section][3][]3}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}NPV and NPF Learning}{5}{subsection.3.1}}
\newlabel{def:switch}{{3.3}{5}{}{definition.3.3}{}}
\newlabel{def:switch@cref}{{[definition][3][3]3.3}{[1][5][]5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Gradient Descent}{5}{subsection.3.2}}
\newlabel{prop:ntknew}{{3.1}{5}{}{proposition.3.1}{}}
\newlabel{prop:ntknew@cref}{{[proposition][1][3]3.1}{[1][5][]5}}
\newlabel{prop:dnnhard}{{3.2}{5}{}{proposition.3.2}{}}
\newlabel{prop:dnnhard@cref}{{[proposition][2][3]3.2}{[1][5][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Deep Gated Networks: Decoupling Neural Path Feature and Value}{5}{section.4}}
\newlabel{sec:decoupled}{{4}{5}{Deep Gated Networks: Decoupling Neural Path Feature and Value}{section.4}{}}
\newlabel{sec:decoupled@cref}{{[section][4][]4}{[1][5][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Deep gated network (DGN) setup. The pre-activations $q^{\text  {F}}_{x,t}(l)$ of layer $l\in [d-1]$ from the feature network are used to derive the gating values $G_{x,t}(l)$ of layer $l\in [d-1]$. \relax }}{6}{figure.caption.5}}
\newlabel{fig:dgn}{{2}{6}{Deep gated network (DGN) setup. The pre-activations $q^{\text {F}}_{x,t}(l)$ of layer $l\in [d-1]$ from the feature network are used to derive the gating values $G_{x,t}(l)$ of layer $l\in [d-1]$. \relax }{figure.caption.5}{}}
\newlabel{fig:dgn@cref}{{[figure][2][]2}{[1][6][]6}}
\newlabel{prop:dgn}{{4.1}{6}{Gradient Dynamics in a DGN}{proposition.4.1}{}}
\newlabel{prop:dgn@cref}{{[proposition][1][4]4.1}{[1][6][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Learning with Fixed NPFs: Role Of Active Sub-Networks}{6}{section.5}}
\newlabel{sec:infomeasure}{{5}{6}{Learning with Fixed NPFs: Role Of Active Sub-Networks}{section.5}{}}
\newlabel{sec:infomeasure@cref}{{[section][5][]5}{[1][6][]6}}
\newlabel{def:gateinfo}{{5.1}{6}{}{definition.5.1}{}}
\newlabel{def:gateinfo@cref}{{[definition][1][5]5.1}{[1][6][]6}}
\citation{ntk,arora2019exact,cao2019generalization}
\citation{arora2019exact}
\citation{arora2019exact}
\citation{arora2019exact}
\citation{arora2019exact}
\citation{arora2019exact}
\newlabel{assmp:main}{{5.1}{7}{}{assumption.5.1}{}}
\newlabel{assmp:main@cref}{{[assumption][1][5]5.1}{[1][7][]7}}
\newlabel{th:main}{{5.1}{7}{}{theorem.5.1}{}}
\newlabel{th:main@cref}{{[theorem][1][5]5.1}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments: Fixed NPFs, NPF Learning and Verification of Claim II}{7}{section.6}}
\newlabel{sec:experiments}{{6}{7}{Experiments: Fixed NPFs, NPF Learning and Verification of Claim II}{section.6}{}}
\newlabel{sec:experiments@cref}{{[section][6][]6}{[1][7][]7}}
\bibstyle{plainnat}
\bibdata{refs}
\bibcite{arora2019exact}{{1}{2019}{{Arora et~al.}}{{Arora, Du, Hu, Li, Salakhutdinov, and Wang}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Dynamics of Learning\relax }}{8}{figure.caption.7}}
\newlabel{fig:dynamics}{{3}{8}{Dynamics of Learning\relax }{figure.caption.7}{}}
\newlabel{fig:dynamics@cref}{{[figure][3][]3}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion and Future Work}{8}{section.7}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Shows the training and generalisation performance of various NPFs.\relax }}{8}{table.caption.8}}
\newlabel{tb:npfs}{{2}{8}{Shows the training and generalisation performance of various NPFs.\relax }{table.caption.8}{}}
\newlabel{tb:npfs@cref}{{[table][2][]2}{[1][8][]8}}
\bibcite{cao2019generalization}{{2}{2019}{{Cao and Gu}}{{}}}
\bibcite{dudln}{{3}{2019}{{Du and Hu}}{{}}}
\bibcite{dudnn}{{4}{2018}{{Du et~al.}}{{Du, Lee, Li, Wang, and Zhai}}}
\bibcite{ntk}{{5}{2018}{{Jacot et~al.}}{{Jacot, Gabriel, and Hongler}}}
\bibcite{shamir}{{6}{2018}{{Shamir}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Related Work}{10}{section.8}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Expression for $K^{(d)}$}{10}{appendix.A}}
\newlabel{sec:kd}{{A}{10}{Expression for $K^{(d)}$}{appendix.A}{}}
\newlabel{sec:kd@cref}{{[appendix][1][2147483647]A}{[1][10][]10}}
\newlabel{eq:ntkold}{{2}{10}{Expression for $K^{(d)}$}{equation.A.2}{}}
\newlabel{eq:ntkold@cref}{{[equation][2][2147483647]2}{[1][10][]10}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Experimental Setup}{10}{appendix.B}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Proofs of technical results}{10}{appendix.C}}
