@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8139--8148},
  year={2019}
}

@inproceedings{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  booktitle={Advances in neural information processing systems},
  pages={8570--8581},
  year={2019}
}

@inproceedings{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10835--10845},
  year={2019}
}

@article{dln,
  title={A convergence theory for deep learning via over-parameterization},
 author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}


@article{ando,
  title={Majorization relations for Hadamard products},
  author={Ando, T},
  journal={Linear algebra and its applications},
  volume={223},
  pages={57--64},
  year={1995},
  publisher={Elsevier Science Publishing Company, Inc.}
}


@article{ben,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}



@article{galu,
  author    = {Jonathan Fiat and
               Eran Malach and
               Shai Shalev{-}Shwartz},
  title     = {Decoupling Gating from Linearity},
  journal   = {CoRR},
  volume    = {abs/1906.05032},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05032},
  archivePrefix = {arXiv},
  eprint    = {1906.05032},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-05032},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@article{ganguli,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@inproceedings{shamir,
  title={Exponential Convergence Time of Gradient Descent for One-Dimensional Deep Linear Neural Networks},
  author={Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={2691--2713},
  year={2019}
}


@article{dudnn,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={arXiv preprint arXiv:1811.03804},
  year={2018}
}


@article{dudln,
  title={Width provably matters in optimization for deep linear neural networks},
  author={Du, Simon S and Hu, Wei},
  journal={arXiv preprint arXiv:1901.08572},
  year={2019}
}


@article{arora,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1901.08584},
  year={2019}
}


@inproceedings{ntk,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}


@article{lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{du2018,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{edgepop,
  title={What's Hidden in a Randomly Weighted Neural Network?},
  author={Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  journal={arXiv preprint arXiv:1911.13299},
  year={2019}
}


@inproceedings{balestriero2018spline,
  title={A spline theory of deep learning},
  author={Balestriero, Randall and others},
  booktitle={International Conference on Machine Learning},
  pages={374--383},
  year={2018}
}



@article{srivastava2014understanding,
  title={Understanding locally competitive networks},
  author={Srivastava, Rupesh Kumar and Masci, Jonathan and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  journal={arXiv preprint arXiv:1410.1165},
  year={2014}
}

@article{sss,
  author    = {Jonathan Fiat and
               Eran Malach and
               Shai Shalev{-}Shwartz},
  title     = {Decoupling Gating from Linearity},
  journal   = {CoRR},
  volume    = {abs/1906.05032},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.05032},
  archivePrefix = {arXiv},
  eprint    = {1906.05032},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1906-05032},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{jacot2019freeze,
  title={Freeze and chaos for dnns: an NTK view of batch normalization, checkerboard and boundary effects},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1907.05715},
  year={2019}
}

@inproceedings{neyshabur2015path,
  title={Path-sgd: Path-normalized optimization in deep neural networks},
  author={Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2422--2430},
  year={2015}
}