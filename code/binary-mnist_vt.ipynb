{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "We train a ReLU network for binary-mnist dataset. We plot $y^T(H_t)^{-1}y$ over time where path gram $H_t = {\\Phi_t}^T{\\Phi_t} = (x^Tx)\\odot\\lambda_t$ and $\\lambda_t(s, s')$ stands for the total number of paths which are active for both input examples $s$ and $s'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt-get install texlive-latex-recommended #1\n",
    "! sudo apt-get install dvipng texlive-fonts-recommended #2\n",
    "! wget http://mirrors.ctan.org/macros/latex/contrib/type1cm.zip #3\n",
    "! unzip type1cm.zip -d /tmp/type1cm #4\n",
    "! cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins  #5\n",
    "! sudo mkdir /usr/share/texmf/tex/latex/type1cm #6\n",
    "! sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm #7\n",
    "! sudo texhash #8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- binaryMnist_galu-vs-ReluThenGalu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras as tfk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "from sklearn.model_selection import *\n",
    "import keras.backend as K\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [-1, 1]\n",
    "def getMnist():\n",
    "    (x_train, y_train), (x_test, y_test) = tfk.datasets.mnist.load_data()\n",
    "    max_x = np.max(x_train)\n",
    "\n",
    "    x_train = x_train / max_x\n",
    "    x_test = x_test / max_x\n",
    "\n",
    "    bool_train = ((y_train == 4) | (y_train == 7))\n",
    "    bool_test = ((y_test == 4) | (y_test == 7))\n",
    "\n",
    "    x_train = x_train[bool_train]\n",
    "    y_train = y_train[bool_train].astype(np.int32)\n",
    "    x_test = x_test[bool_test]\n",
    "    y_test = y_test[bool_test].astype(np.int32)\n",
    "    y_train[y_train == 4] = labels[0]\n",
    "    y_test[y_test == 4] = labels[0]\n",
    "    y_train[y_train == 7] = labels[1]\n",
    "    y_test[y_test == 7] = labels[1]\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = getMnist()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEig(g_mat):\n",
    "    eig_vals = np.linalg.eig(g_mat)\n",
    "    eig_vals = np.real(eig_vals[0])\n",
    "    return eig_vals\n",
    "\n",
    "def getRandomIndices(max_lim, num_ind):\n",
    "    all_indices  = np.arange(0, max_lim)\n",
    "    np.random.shuffle(np.arange(0, max_lim))\n",
    "    return all_indices[: num_ind]\n",
    "\n",
    "def getUniqueLabelExamples(x_train, y_train, num_class, num_each_class):\n",
    "    x_batch = x_train[:num_each_class * num_class,:].copy()\n",
    "    y_batch = np.zeros((num_each_class * num_class, ))\n",
    "\n",
    "    for ind, label in enumerate(labels):\n",
    "        x_temp = x_train[(y_train == label), :]\n",
    "        max_lim = x_temp.shape[0]\n",
    "        indices = getRandomIndices(max_lim, num_each_class)\n",
    "        x_batch[ind*num_each_class: (ind+1)*num_each_class, :] = x_temp[indices, :]\n",
    "        y_batch[ind*num_each_class: (ind+1)*num_each_class, ] = label\n",
    "\n",
    "    return x_batch, y_batch\n",
    "\n",
    "sess = K.get_session()\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def getLayersOutput(wts, X):\n",
    "    layer_output = [X]\n",
    "    for i, w in enumerate(wts[:-1]):\n",
    "        # print(X.shape)\n",
    "        output = relu(np.matmul(X, w))\n",
    "        layer_output.append(output)\n",
    "        X = output\n",
    "    w = wts[-1]\n",
    "    # print(X.shape, w.shape)\n",
    "    output = np.matmul(X, w)\n",
    "    layer_output.append(output)\n",
    "    return layer_output    \n",
    "\n",
    "def getLayerActivation(wts, X):\n",
    "    layer_outputs = getLayersOutput(wts, X)\n",
    "    layer_activity = []\n",
    "    for output in layer_outputs[1:-1]:\n",
    "        activity = (output > 0).astype(np.int)\n",
    "        layer_activity.append(activity)\n",
    "    return layer_activity\n",
    "\n",
    "def getActivityGram(layer_activity):\n",
    "    num_input = layer_activity[0].shape[0]\n",
    "    layer_gram = []\n",
    "    total_gram = np.ones((num_input, num_input))\n",
    "    for activity in layer_activity:\n",
    "        gram = np.dot(activity, activity.T)\n",
    "        total_gram = np.multiply(total_gram, gram)\n",
    "\n",
    "    return total_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 100\n",
    "depth = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sz = 784\n",
    "output_sz = 1\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "num_exp = 5\n",
    "\n",
    "def my_init2(shape, dtype = None):\n",
    "    return tf.Variable(np.random.choice([-np.sqrt(2/width), np.sqrt(2/width)], \n",
    "                                        shape, [0.5, 0.5]), dtype = dtype, trainable= True)\n",
    "\n",
    "def getRelu():\n",
    "    inputs = tfk.Input(shape = (input_sz, ))\n",
    "    R1 = Dense(units = width, activation = 'relu', use_bias = False, \n",
    "               kernel_initializer = my_init2, name = \"R1\")(inputs)\n",
    "\n",
    "    for i in range(depth - 2):\n",
    "        R1 = Dense(units = width, activation = 'relu', use_bias = False, \n",
    "                   kernel_initializer = my_init2, name = \"R\"+str(i+2))(R1)\n",
    "\n",
    "    outputs = Dense(units = output_sz, activation = \"linear\", use_bias = False,\n",
    "                    kernel_initializer = my_init2, name = \"R\"+str(depth))(R1)\n",
    "    model = tfk.Model(inputs = inputs, outputs = outputs, name = 'mnist_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_relu = {'mean_squared_error' : [], 'val_mean_squared_error' : [], 'norm_normalized' : [], \n",
    "                'norm_unnormalized' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = getUniqueLabelExamples(x_train, y_train, 2, 100)\n",
    "input_gram = np.dot(x_batch, x_batch.T)\n",
    "print(x_batch.shape, y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_i in range(num_exp):\n",
    "    model = getRelu()\n",
    "\n",
    "    optimizer = tfk.optimizers.RMSprop(lr = 1e-4)\n",
    "    model.compile(optimizer= optimizer, loss = 'mse', metrics = ['mse'])\n",
    "    model.fit(x_train[:1], y_train[:1], epochs = 1, batch_size = 1, verbose = 0)\n",
    "\n",
    "    mse_temp = []\n",
    "    val_mse_temp = []\n",
    "    norm_temp_normalized = []\n",
    "    norm_temp_unnormalized = []\n",
    "\n",
    "    for epoch_i in (range(num_epochs)):\n",
    "        if epoch_i % 2 == 0:\n",
    "            wts = model.get_weights()\n",
    "            layer_activity = getLayerActivation(wts, x_batch)\n",
    "            K_t = np.multiply(input_gram, getActivityGram(layer_activity))\n",
    "            K_t_normalized = K_t / np.sum(np.diagonal(K_t))\n",
    "\n",
    "            norm = np.dot(y_batch, np.dot(np.linalg.inv(K_t), y_batch))\n",
    "            norm_temp_unnormalized.append(norm)\n",
    "\n",
    "            norm = np.dot(y_batch, np.dot(np.linalg.inv(K_t_normalized), y_batch))\n",
    "            norm_temp_normalized.append(norm)\n",
    "\n",
    "        history = model.fit(x_train, y_train, validation_data=(x_test, y_test), \n",
    "                                    epochs = 1, batch_size = batch_size, verbose = 0)\n",
    "        \n",
    "        mse_temp.append(history.history['mean_squared_error'][0])\n",
    "        val_mse_temp.append(history.history['val_mean_squared_error'][0])\n",
    "\n",
    "    history_relu['mean_squared_error'].append(mse_temp)\n",
    "    history_relu['val_mean_squared_error'].append(val_mse_temp)\n",
    "    history_relu['norm_normalized'].append(norm_temp_normalized)\n",
    "    history_relu['norm_unnormalized'].append(norm_temp_unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_norm = np.mean(history_relu['norm_normalized'], axis = 0)\n",
    "mean_std = np.std(history_relu['norm_normalized'], axis = 0)\n",
    "plt.errorbar(x = np.arange(mean_norm.shape[0]), y = mean_norm, yerr = mean_std, marker = 'o')\n",
    "plt.ylabel(\"norm\", fontsize = 14)\n",
    "plt.xlabel(\"epochs\", fontsize = 14)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
