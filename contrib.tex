\subsection{Our Contributions}\label{sec:contrib}
To the best of our knowledge, we are the first to analytically characterise the role played by active sub-networks in deep learning as presented in the `Claims I and II'.  The key contributions can be arranged into three landmarks as described below.

$\bullet$ The first step involves breaking a DNN into individual paths, and each path again into gates and weights.  To this end, we encode the states of the gates in a novel \emph{neural path feature} (NPF) and the weights in a novel \emph{neural path value} (NPV) and express the output of the DNN as an inner product of NPF and NPV (see \Cref{sec:path}). In contrast to NTF/NTK which are \emph{first-order} quantities (based on derivatives with respect to the weights), NPF and NPV are \emph{zeroth-order} quantities. The kernel matrix associated to the NPFs namely the \emph{neural path kernel} (NPK) matrix $H_{\Theta}\in\R^{n\times n}$ has a special structure, i.e., it can be written as a \emph{Hadamard} product of the input Gram matrix, and a correlation matrix $\Lambda_{\Theta}\in\R^{n\times n}$, whose entries $\Lambda_{\Theta}(s,s')$ is equal to the total number of path in the sub-network that is active for both input examples $s,s'\in[n]$. With the $\Lambda_{\Theta}$ matrix we reach our first landmark.
%$H_{\Theta}=\Sigma\odot\Lambda_{\Theta}$, where $\odot$ is the \emph{Hadamard} product, $\Sigma\in\R^{n\times n}$ is the input Gram matrix and $\Lambda_{\Theta}\in\R^{n\times n}$ is a correlation matrix, whose entries $\Lambda_{\Theta}(s,s')$ is equal to the total number of path in the sub-network that is active for both input examples $s,s'\in[n]$. With the $\Lambda_{\Theta}$ matrix we reach our first landmark.%To the best of our knowledge quantities NPF, NPV, NPK and $\Lambda$ are novel and have not appeared in prior work. These novels quantities help us to analytically establish the fact that the active sub-networks play a critical role in deep learning, a fact which was not established analytically in prior works.

$\bullet$  Second step is to characterise performance of active sub-networks in a `stand alone' manner. To this end, we consider a new idealised setting namely fixed NPF (FNPF) setting, wherein, the NPFs are fixed (i.e., held constant) and only the NPV is learnt via gradient descent. In this setting, we show that (see \Cref{th:main}), in the limit of infinite width and under randomised initialisation the NTK converges to a matrix $K^{(d)}_{\text{FNPF}}=\text{constant} \times H_{\text{FNPF}}$, where $H_{\text{FNPF}}\in\R^{n\times n}$ is the NPK matrix corresponding to the fixed NPFs. $K^{(d)}$ matrix of \cite{ntk,arora2019exact,cao2019generalization} becomes the $K^{(d)}_{\text{FNPF}}$ matrix in the FNPF setting,  wherein, we initialise the NPV statistically independent of the fixed NPFs (see \Cref{assmp:main}). With \Cref{th:main}, we reach our second landmark, i.e. we justify ``Claim I'', that active sub-networks are fundamental entities, which follows from the fact that $H_{\text{FNPF}}=\Sigma\odot \Lambda_{\text{FNPF}}$, where $\Lambda_{\text{FNFP}}$ corresponds to the fixed NPFs.

$\bullet$ Third step is to show experimentally that sub-network learning happens in practice. We show that in finite width DNNs with ReLU activations, NPFs are learnt continuously during training, and such learning is key for generalisation. We observe that fixed NPFs obtained from the initial stages of training generalise poorly than CNTK (of \cite{arora2019exact}), whereas, fixed NPFs obtained from later stages of training generalise better than CNTK and generalise as well as standard DNNs with ReLU. This throws light on the open question in \Cref{sec:background}, i.e., the difference between the NTK regime and the finite width DNNs is perhaps due to NPF learning. In finite width DNNs, NPFs are learnt during training and in the NTK regime no such feature learning happens during training (since $K^{(d)}$ is fixed). Since the NPFs completely encode the information pertaining to the active sub-networks, we complete our final landmark namely  justification of ``Claim II'', 
\begin{comment}
$\bullet$ \textbf{Gate Encoding (\Cref{sec:path}):}  We encode the states of the gates in a novel \emph{neural path feature} (NPF) and the weights in a novel \emph{neural path value} (NPV) and express the output of the DNN as an inner product of NPF and NPV. In contrast to NTF/NTK which are \emph{first-order} quantities (based on derivatives with respect to the weights), NPF and NPV are \emph{zeroth-order} quantities.% and the \emph{first-order} gradient based (d quantities namely NTF/NTK can be derived using NPF and NPV.

$\bullet$ \textbf{Gate Encoding:} We introduce three novel quantities namely the \emph{neural path feature} (NPF), the \emph{neural path value} (NPV), and the \emph{neural path kernel} (NPK) (see \Cref{sec:path}). The NPF encodes the states of the gates and the NPV encodes the weights, and we write down the output of the DNN as an inner product of the NPF and the NPV. The NPF and NPV are \emph{primitive/fundamental} quantities, in that, they are zeroth-order quantities, and the first-order gradient based quantities namely NTF/NTK can be derived using NPF and NPV.
%The NPFs and NPV are learnt during gradient descent (see \Cref{sec:gatedyna}).%We write down the dynamics of GD where we explictly  with the NPFs and NPV learning (\Cref{prop:npflearn}).%The NPF and NPV are \emph{primitive/fundamental} quantities, in that, they are zeroth-order quantities, and the first-order gradient based quantities namely NTF/NTK can be derived using NPF and NPV. %We write down the dynamics of gradient descent in which the NPFs and NPV are learnt simultaneously (\Cref{prop:npflearn}).

$\bullet$ \textbf{Gate Dynamics (\Cref{sec:gatedyna}):}  The derivative of the ReLU gate with respect to its pre-activation $\frac{d\gamma_{r}(q)}{dq}=0$ is almost everywhere. Thus, the sensitivity of the gates to the DNN weights is $0$, and hence the dynamics of the gates is not explicitly captured in prior works. We use  `soft-ReLU'  given by $\gamma_{sr}(q)=\frac{1}{\left(1+\exp(-\beta \cdot q)\right)}, \beta>0$, which is differentiable with respect to $q$ and capture the changes in the gates, i.e., change of the NPFs during GD.

$\bullet$ \textbf{Gate Decoupling (\Cref{sec:decoupled}):}  We introduce a deep gated network (DGN) framework, wherein, the NPF and the NPV are decoupled by storing them in two separate networks. This enables us to fix the NPFs during training (i.e., keep them constant) and learn only the NPVs via GD, and measure the information stored in the gates/NPFs. We also use the DGN framework to learn NPFs and NPV in a `decoupled' manner using two separate set of weights, and demonstrate the power of NPF learning.%the performance of this `decoupled' learner we demonstrate the power of NPF learning.

$\bullet$ \textbf{Interpretable Kernel (\Cref{sec:infomeasure}):} We show that in the case of training with fixed NPFs, in the limit of infinite width, the NTK matrix can be simplified and the NTK is equal to the NPK times a constant (see \Cref{th:main}). The fixed NPF setting needs tighter assumptions (\Cref{assmp:main}) than the prior works on NTK. However, what we lose in generality, we gain in \emph{interpretability}. The NPK can be decomposed as a \emph{Hadamard} product of the input Gram matrix, and a correlation matrix that measures the fractional overlap of active sub-networks all possible pairs of examples in the dataset.  This justifies our ``Claim I''.

$\bullet$ \textbf{NPF Learning (\Cref{sec:experiments}) :} We show that in finite width DNNs with ReLU activations, NPFs are learnt continuously during training, and such learning is key for generalisation. We observe that fixed NPFs obtained from the initial stages of training generalise poorly than CNTK (of \cite{arora2019exact}), whereas, fixed NPFs obtained from later stages of training generalise better than CNTK and generalise as well as standard DNNs with ReLU. This throws light on the open question in \Cref{sec:background}, i.e., the difference between the NTK regime and the finite width DNNs is perhaps due to NPF learning. In finite width DNNs, NPFs are learnt during training and in the NTK regime no such feature learning happens during training (since $K^{(d)}$ is fixed). This justifies our ``Claim II''.

\end{comment}


