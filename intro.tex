\section{Introduction}
We consider deep neural networks (DNNs) with rectified linear unit (ReLU) activations. A special property of ReLU activation is that it can be written as a product of its pre-activation input, say $q\in\R$ and a gating signal, $\gamma_r(q)=\mathbbm{1}_{\{q>0\}}$, i.e., $\chi_r(q)=q\cdot\gamma_r(q)$. In what follows, we call $\chi_r$ as the ReLU activation and $\gamma_r$ as the ReLU gate.  While the weights (of a DNN) remain the same across input examples, the $1/0$ state of the gates (or simply gates) change across input examples. For each input example, there is a corresponding \emph{active} sub-network consisting of those gates which are $1$, and the weights which pass through such gates. This active sub-network can be said to hold the memory for a given input, i.e., only those weights that pass through such active gates contribute to the output. In this viewpoint, at random initialisation of the weights, for a given input example, a random sub-network is active and produces a random output.  However, as the weights change during training (say via gradient descent), the gates change, and hence the active sub-networks corresponding to the various input examples also change. At the end of training, for each input example, there is a learned active sub-network, and produces a learned output. Thus, the gates of a trained DNN could potentially contain valuable information. In this paper, we study the role of the gates, and the dynamics of the gates while training DNNs  using gradient descent (GD). Our findings can be summarised in the following claims which we theoretically/experimentally justify in the paper:

Claim I  (see \Cref{sec:infomeasure}): \emph{Active sub-networks are fundamental entities in DNNs with ReLU activations.}  

Claim II (see \Cref{sec:experiments}): \emph{Learning of the active sub-networks during training is key for generalisation.}

Before we discuss ``Claims I and II'' in terms of our novel contributions in \Cref{sec:contrib}, we present the background of  \emph{neural tangent feature and kernel} (NTF and NTK) in \Cref{sec:background}.

\textbf{Notation:} We denote the set $\{1,\ldots, n\}$ by $[n]$. For $x,y\in\R^m$, $\ip{x,y}=x^\top y$. The maximum and minimum eigenvalue of a real symmetric matrix $A$ are denoted by $\rho_{\max}(A)$ and $\rho_{\min}(A)$. We consider fully-connected DNNs with $w$ hidden units per layer and $d-1$ hidden layers. The output of the DNN for an input $x\in\R^{d_{in}}$ is denoted by $\hat{y}_{\Theta}(x)\in\R$, where $\Theta\inrdnet$ are the network weight ($d_{net}=d_{in}w+(d-2)w^2+w$). We denote by $\Theta(l,j,i)$, the weight connecting the $j^{th}$ hidden unit of layer $l-1$ to the $i^{th}$ hidden unit of layer $l\in[d]$. $\Theta(1)\in\R^{w\times d_{in}}, \Theta(l)\in\R^{w\times w},\forall l\in\{2,\ldots,d-1\}, \Theta(d)\in\R^{w\times 1}$. The dataset is given by $(x_s,y_s)_{s=1}^n\in\R^{d_{in}}\times \R$. The loss function is given by $L_{\Theta}=\frac{1}{2}\sum_{s=1}^n \left(\hat{y}_{\Theta}(x_s)-y_s\right)^2$. We use $\nabla_{\Theta}(\cdot)$ stands for the gradient of $(\cdot)$ with respect to the network weights. We use vectorised notations $y=(y_s,s\in[n]), \hat{y}_{\Theta}=\left(\hat{y}_{\Theta}(x_s), s\in[n]\right)\in\R^n$ for the true and predicted outputs and $e_{t}= (\hat{y}_{\Theta_t}-y)\in\R^n$ for the error in the prediction. We use $\theta\in\Theta$ to denote single arbitrary weight, and $\partial_{\theta}(\cdot)$ to denote $\frac{\partial (\cdot)}{\partial \theta}$.  $\Sigma\in\R^{n\times n}$ is the input Gram matrix with entries $\Sigma(s,s')=\ip{x_s,x_{s'}}$.
%We consider the gradient descent update given by $\Theta_t=\Theta_t-\alpha_t (\nabla_{\Theta} L_{\Theta_t})$, where $\alpha_t>0$ is a small step-size and 
\input{background}
\input{contrib}