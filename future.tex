\section{Conclusion}
\begin{comment}
Optimisation and generalisation of DNNs has been an important question in machine learning. Prior works have considered the \emph{neural tangent features} (NTF) and an associated \emph{neural tangent kernel} (NTK), which are quantities based on the first-order gradient information.  In this paper, we looked at fully connected DNNs with ReLU activations, and for such networks, we defined a novel feature namely the \emph{neural path feature} (NPF) and an associated \emph{neural path kernel} , which are \emph{zeroth-order} quantities. The NPFs are based on the information in the gates of a DNN, and hence the NPFs change as the network is trained. We theoretically showed that NPK can be used to understand the optimisation and generalisation for models trained with fixed NPFs. We showed in the experiments that in standard DNNs with ReLU activations, NPFs are learnt during training, and such NPF learning is key for generalisation performance. We also showed via experiments that deep gated networks, where NPFs are learned in a decoupled manner also generalise well. 

A possible future direction is to understand the role of depth and width in NPF learning, and the role of NPF in generalisation. The deep gated network might be useful in this effort, since the NPFs learning is decoupled and is perhaps amenable to analysis in comparison the standard DNNs.
\end{comment}
In this paper, we studied the role of active sub-networks in deep learning by encoding the gates in the neural path features. We showed that the neural path features are learnt during training and such learning is key for generalisation. In our experiments, we observed that almost all information of a trained DNN is stored in the neural path features. We conclude by saying that \emph{understanding deep learning requires understanding neural path feature learning}. %The deep gated network whose gates are decoupled is perhaps an easier starting point to analyse neural path feature learning. 
%A possible future direction is to understand the role of depth and width in NPF learning, and the role of NPF in generalisation.
\begin{comment}
In this paper, we considered the problem of optimisation and generalisation in DNNs with ReLU activations. Throughout the paper, we exploited the special property of the ReLU activation, in that, i output of a ReLU activation be expressed as a product of its pre-activation input and its gating value which is $1/0$ based on whether or not the pre-activation is positive or negative. To analyse such networks, we  introduced the `path-view': a path starts from the input passes through one weight and one hidden nodes per layer and finally ends in the output, and the output of a network is seen as a cumulative combination of the contributions from various paths. The computation in each path was further divided into those happening in the weights, and those happening in the gates. This enabled us to express the output of such DNNs as an inner product of neural path feature (for a path, its feature is the product of gates it encounters from input to output)  and neural path value (for a path, its value is the product of weights it encounters from input to output). The neural path feature (NPF) of a given input is completely dictated by the gating pattern, i.e., the \emph{on/off} status of the gates in the network for that input. Due to the \emph{on/off} nature of the gates, their change to infinitesimal change in the network parameters is $0$, i.e., the gradient of the NPFs with respect to the network weights is $0$. However, in practice the NPFs change during training. By considering a soft-ReLU activation, which served as a `differentiable' substitute for the ReLU activation, we could capture the feature gradient, i.e., the gradient of the NPFs with respect to the network weights. This enabled us to write down the gradient descent dynamics that incorporated NPF learning. We presented the following interesting results:

$1.$ In the fixed NPF regime, wherein, the NPFs are held constant through training, the optimisation and generalisation depends on the associated neural path kernel.

$2.$ 
\end{comment}
