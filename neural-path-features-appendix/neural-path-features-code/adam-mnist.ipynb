{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import *\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import re\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignGate(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SignGate, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(SignGate, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        output = K.sign(K.relu(x))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def getNetwork1Layers(model):\n",
    "    pattern = re.compile(\"^R[0-9]*$\")\n",
    "    layer_list  = []\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if pattern.match(layer.name):\n",
    "            layer_list.append(layer)\n",
    "\n",
    "    return layer_list\n",
    "\n",
    "def getNetwork2Layers(model):\n",
    "    pattern = re.compile(\"^G[0-9]*$\")\n",
    "    layer_list  = []\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if pattern.match(layer.name):\n",
    "            layer_list.append(layer)\n",
    "\n",
    "    return layer_list\n",
    "\n",
    "def getGalu(depth, width):\n",
    "    inputs = Input(shape = (784, ))\n",
    "\n",
    "    R1 = Dense(units = width, activation = 'relu', name = \"R1\")(inputs)\n",
    "    G1 = Dense(units = width, activation = 'linear',  name = \"G1\")(inputs)\n",
    "    G1Activ = SignGate(name = \"G1Activ\")(R1)\n",
    "    G1A = Multiply(name = \"G1A\")([G1, G1Activ])\n",
    "\n",
    "    for i in range(depth - 2):\n",
    "        R1 = Dense(units = width, activation = 'relu',name = \"R\"+str(i+2))(R1)\n",
    "        G1 = Dense(units = width, activation = 'linear', name = \"G\"+str(i+2))(G1A)\n",
    "        G1Activ = SignGate(name = \"G\"+str(i+2)+\"Activ\")(R1)\n",
    "        G1A = Multiply(name = \"G\"+str(i+2)+\"A\")([G1, G1Activ])\n",
    "\n",
    "    outputs = Dense(units = 10, activation = \"softmax\", name = \"G\"+str(depth))(G1A)\n",
    "    model = keras.Model(inputs = inputs, outputs = outputs, name = 'galu_model')\n",
    "\n",
    "    return model\n",
    "\n",
    "def getRelu(depth, width):\n",
    "    inputs = Input(shape = (784, ))\n",
    "    R1 = Dense(units = width, activation = 'relu', name = \"R1\")(inputs)\n",
    "    for i in range(depth - 2):\n",
    "        R1 = Dense(units = width, activation = 'relu', name = \"R\"+str(i+2))(R1)\n",
    "\n",
    "    outputs = Dense(units = 10, activation = \"softmax\", name = \"R\"+str(depth))(R1)\n",
    "\n",
    "    model = keras.Model(inputs = inputs, outputs = outputs, name = 'relu_model')\n",
    "\n",
    "    return model\n",
    "\n",
    "eps, beta = 0.1, 4\n",
    "\n",
    "class SoftGate(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftGate, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(SoftGate, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        activation = (1 + eps)*K.sigmoid(beta*x)\n",
    "        return activation\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def getDecoupledLearning(depth, width):\n",
    "    inputs = Input(shape = (784, ))\n",
    "\n",
    "    R1 = Dense(units = width, activation = 'linear', name = \"R1\")(inputs)\n",
    "    R1A = Activation('relu')(R1)\n",
    "    A1 = SoftGate()(R1)\n",
    "\n",
    "    G1 = Dense(units = width, activation = 'linear',  name = \"G1\")(inputs)\n",
    "    G1A = Multiply(name = \"G1A\")([G1, A1])\n",
    "\n",
    "    for i in range(depth - 2):\n",
    "        R1 = Dense(units = width, activation = 'linear',name = \"R\"+str(i+2))(R1A)\n",
    "        R1A = Activation('relu')(R1)\n",
    "        A1 = SoftGate()(R1)\n",
    "\n",
    "        G1 = Dense(units = width, activation = 'linear', name = \"G\"+str(i+2))(G1A)\n",
    "        G1A = Multiply(name = \"G\"+str(i+2)+\"A\")([G1, A1])\n",
    "\n",
    "    outputs = Dense(units = 10, activation = \"softmax\", name = \"G\"+str(depth))(G1A)\n",
    "    model = keras.Model(inputs = inputs, outputs = outputs, name = 'galu_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth, width = 6, 128\n",
    "lr = 1e-4\n",
    "loss = keras.losses.categorical_crossentropy\n",
    "batch_size = 32\n",
    "num_exp = 5\n",
    "num_epochs = 100\n",
    "\n",
    "history_relu = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n",
    "history_galu_learned = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n",
    "history_frozen_relu = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n",
    "history_galu = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n",
    "history_decoupled_learning = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.**Train Relu & GaLU Learned**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_i in range(num_exp):\n",
    "    print(\"_____________EXP:{}____________\".format(exp_i+1))\n",
    "\n",
    "    model_relu = getRelu(depth, width)\n",
    "    model_relu.compile(loss = loss, optimizer = keras.optimizers.Adam(lr), metrics = ['acc'])\n",
    "\n",
    "    filepath=\"weights.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    history = model_relu.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n",
    "                                batch_size=batch_size, epochs= num_epochs, callbacks = callbacks_list)\n",
    "    history_relu['acc'].append(history.history['acc'])\n",
    "    history_relu['val_acc'].append(history.history['val_acc'])\n",
    "    history_relu['loss'].append(history.history['loss'])\n",
    "    history_relu['val_loss'].append(history.history['val_loss'])\n",
    "    print(\"ReLU: MAX ACC = {}, MAX VAL ACC = {}\".format(np.max(history.history['acc']), \n",
    "                                                        np.max(history.history['val_acc'])))\n",
    "    model_relu.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "    model_galu_learned = getGalu(depth, width)\n",
    "    layers_relu = getNetwork1Layers(model_relu)[:-1]\n",
    "    layers_galu_learned = getNetwork1Layers(model_galu_learned)\n",
    "\n",
    "    for layer in layers_galu_learned:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model_galu_learned.compile(loss = loss, optimizer = keras.optimizers.Adam(lr), metrics = ['acc'])\n",
    "\n",
    "    for layer1,layer2 in zip(layers_galu_learned, layers_relu): \n",
    "        layer1.set_weights(layer2.get_weights())\n",
    "\n",
    "    history = model_galu_learned.fit(x_train, y_train,validation_data = (x_test, y_test), verbose = 0,\n",
    "                                     batch_size=batch_size, epochs= num_epochs)\n",
    "    history_galu_learned['acc'].append(history.history['acc'])\n",
    "    history_galu_learned['val_acc'].append(history.history['val_acc'])\n",
    "    history_galu_learned['loss'].append(history.history['loss'])\n",
    "    history_galu_learned['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "    print(\"GaLU Learned: MAX ACC = {}, MAX VAL ACC = {}\".format(np.max(history.history['acc']), \n",
    "                                                        np.max(history.history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ReLU: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n",
    "                                                    np.mean(np.max(history_relu['acc'], axis = 1)), \n",
    "                                                    np.mean(np.max(history_relu['val_acc'], axis = 1)),\n",
    "                                                    np.std(np.max(history_relu['val_acc'], axis = 1))))\n",
    "\n",
    "print(\"GaLU Learned: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n",
    "                                                    np.mean(np.max(history_galu_learned['acc'], axis = 1)), \n",
    "                                                    np.mean(np.max(history_galu_learned['val_acc'], axis = 1)),\n",
    "                                                    np.std(np.max(history_galu_learned['val_acc'], axis = 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('h_mnist_arch1_relu', 'wb')\n",
    "pickle.dump(history_relu, file)\n",
    "\n",
    "file = open('h_mnist_arch1_galu_learned', 'wb')\n",
    "pickle.dump(history_galu_learned, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Frozen ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_i in range(num_exp):\n",
    "    print(\"_____________EXP:{}____________\".format(exp_i+1))\n",
    "    model_frozen_relu = getGalu(depth, width)\n",
    "    layers_frozen_relu_n1 = getNetwork1Layers(model_frozen_relu)\n",
    "    layers_frozen_relu_n2 = getNetwork2Layers(model_frozen_relu)\n",
    "\n",
    "    for layer in layers_frozen_relu_n1:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model_frozen_relu.compile(loss = loss, optimizer = keras.optimizers.Adam(lr), metrics = ['acc'])\n",
    "\n",
    "    for layer1, layer2 in zip(layers_frozen_relu_n1, layers_frozen_relu_n2):\n",
    "        layer1.set_weights(layer2.get_weights())    \n",
    "\n",
    "    history = model_frozen_relu.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n",
    "                                   batch_size=batch_size, epochs= num_epochs)\n",
    "    history_frozen_relu['acc'].append(history.history['acc'])\n",
    "    history_frozen_relu['val_acc'].append(history.history['val_acc'])\n",
    "    history_frozen_relu['loss'].append(history.history['loss'])\n",
    "    history_frozen_relu['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "    print(\"Frozen ReLU: MAX ACC = {:.4f}, MAX VAL ACC = {:.4f}\".format(np.max(history.history['acc']), \n",
    "                                                        np.max(history.history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frozen ReLU: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n",
    "                                                    np.mean(np.max(history_frozen_relu['acc'], axis = 1)), \n",
    "                                                    np.mean(np.max(history_frozen_relu['val_acc'], axis = 1)),\n",
    "                                                    np.std(np.max(history_frozen_relu['val_acc'], axis = 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('h_mnist_arch1_frozen_relu', 'wb')\n",
    "pickle.dump(history_frozen_relu, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **GaLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_i in range(num_exp):\n",
    "    print(\"_____________EXP:{}____________\".format(exp_i+1))\n",
    "    model_galu = getGalu(depth, width)\n",
    "    layers_frozen_relu_n1 = getNetwork1Layers(model_galu)\n",
    "\n",
    "    for layer in layers_frozen_relu_n1:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model_galu.compile(loss = loss, optimizer = keras.optimizers.Adam(lr), metrics = ['acc'])\n",
    "\n",
    "    history = model_galu.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n",
    "                             batch_size=batch_size, epochs= num_epochs)\n",
    "    history_galu['acc'].append(history.history['acc'])\n",
    "    history_galu['val_acc'].append(history.history['val_acc'])\n",
    "    history_galu['loss'].append(history.history['loss'])\n",
    "    history_galu['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "    print(\"GaLU: MAX ACC = {:.4f}, MAX VAL ACC = {:.4f}\".format(np.max(history.history['acc']), \n",
    "                                                        np.max(history.history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GaLU: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n",
    "                                                    np.mean(np.max(history_galu['acc'], axis = 1)), \n",
    "                                                    np.mean(np.max(history_galu['val_acc'], axis = 1)),\n",
    "                                                    np.std(np.max(history_galu['val_acc'], axis = 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('h_mnist_arch1_history_galu', 'wb')\n",
    "pickle.dump(history_galu, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decoupled Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_i in range(num_exp):\n",
    "    print(\"_____________EXP:{}____________\".format(exp_i+1))\n",
    "    model_dl = getDecoupledLearning(depth, width)\n",
    "    model_dl.compile(loss = loss, optimizer = keras.optimizers.Adam(lr), metrics = ['acc'])\n",
    "\n",
    "    history = model_dl.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n",
    "                             batch_size=batch_size, epochs= num_epochs)\n",
    "    \n",
    "    history_decoupled_learning['acc'].append(history.history['acc'])\n",
    "    history_decoupled_learning['val_acc'].append(history.history['val_acc'])\n",
    "    history_decoupled_learning['loss'].append(history.history['loss'])\n",
    "    history_decoupled_learning['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "    print(\"Decoupled Learning: MAX ACC = {:.4f}, MAX VAL ACC = {:.4f}\".format(\n",
    "                                                        np.max(history.history['acc']), \n",
    "                                                        np.max(history.history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decoupled Learning: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n",
    "                                            np.mean(np.max(history_decoupled_learning['acc'], axis = 1)), \n",
    "                                            np.mean(np.max(history_decoupled_learning['val_acc'], axis = 1)),\n",
    "                                            np.std(np.max(history_decoupled_learning['val_acc'], axis = 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('h_mnist_decoupled_learning', 'wb')\n",
    "pickle.dump(history_decoupled_learning, file)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
