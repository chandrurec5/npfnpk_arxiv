{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as tfk\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tqdm import *\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tfk.datasets.cifar10.load_data()\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignGate(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SignGate, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(SignGate, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        output = K.sign(K.relu(x))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def getNetwork1Layers(model):\n",
    "    pattern = re.compile(\"^n1_*\")\n",
    "    layer_list  = []\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if pattern.match(layer.name):\n",
    "            layer_list.append(layer)\n",
    "\n",
    "    return layer_list\n",
    "\n",
    "def getNetwork2Layers(model):\n",
    "    pattern = re.compile(\"^G[0-9]*$\")\n",
    "    layer_list  = []\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if pattern.match(layer.name):\n",
    "            layer_list.append(layer)\n",
    "\n",
    "    return layer_list\n",
    "\n",
    "def freezeWeights(layers):\n",
    "    for layer in layers:\n",
    "        print(layer.name)\n",
    "        layer.trainable = False\n",
    "\n",
    "img_wid = 32\n",
    "def getConv4Relu():\n",
    "    inputs = Input(shape = (img_wid, img_wid, 3))\n",
    "    \n",
    "    C1 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c1\",\n",
    "                           activation = 'relu')(inputs)\n",
    "    C2 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c2\",\n",
    "                           activation = 'relu')(C1)\n",
    "    C3 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c3\",\n",
    "                           activation = 'relu')(C2)\n",
    "    C4 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c4\",\n",
    "                           activation = 'relu')(C3)\n",
    "\n",
    "    G1 = GlobalAveragePooling2D()(C4)\n",
    "    F1 = Flatten()(G1)\n",
    "    D1 = Dense(units = 256, activation = 'relu', name = \"n1_d1\")(F1)\n",
    "    D2 = Dense(units = 256, activation = 'relu', name = \"n1_d2\")(D1)\n",
    "\n",
    "    #Output\n",
    "    outputs = Dense(units = 10, activation = 'softmax', name = \"n1_o1\")(D2)\n",
    "\n",
    "    model = tfk.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def getConv4Galu():\n",
    "    inputs = Input(shape = (img_wid, img_wid, 3))\n",
    "    #V1\n",
    "    C1 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c1\",\n",
    "                           activation = 'relu')(inputs)\n",
    "    C2 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c2\",\n",
    "                           activation = 'relu')(C1)\n",
    "    C3 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c3\",\n",
    "                           activation = 'relu')(C2)\n",
    "    C4 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c4\",\n",
    "                           activation = 'relu')(C3)\n",
    "    G1 = GlobalAveragePooling2D()(C4)\n",
    "    F1 = Flatten()(G1)\n",
    "    D1 = Dense(units = 256, activation = 'relu', name = \"n1_d1\")(F1)\n",
    "    D2 = Dense(units = 256, activation = 'relu', name = \"n1_d2\")(D1)\n",
    "\n",
    "    A1 = SignGate()(C1)\n",
    "    A2 = SignGate()(C2)\n",
    "    A3 = SignGate()(C3)\n",
    "    A4 = SignGate()(C4)\n",
    "    A5 = SignGate()(D1)\n",
    "    A6 = SignGate()(D2)\n",
    "    C1_G = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n2_c1\",\n",
    "                           activation = 'linear')(inputs)\n",
    "    C1_G = Multiply()([A1, C1_G])\n",
    "\n",
    "    C2_G = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n2_c2\", \n",
    "                           activation = 'linear')(C1_G)\n",
    "    C2_G = Multiply()([A2, C2_G])\n",
    "\n",
    "    #V2\n",
    "    C3_G = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n2_c3\",\n",
    "                           activation = 'linear')(C2_G)\n",
    "    C3_G = Multiply()([A3, C3_G])\n",
    "\n",
    "    C4_G = Conv2D(filters = 128, kernel_size = (3, 3),  padding = 'same', name = \"n2_c4\",\n",
    "                           activation = 'linear')(C3_G)\n",
    "    C4_G = Multiply()([A4, C4_G])\n",
    "\n",
    "    G1_G = GlobalAveragePooling2D()(C4_G)\n",
    "    F1_G = Flatten()(G1_G)\n",
    "\n",
    "    D1_G = Dense(units = 256, activation = 'linear', name = \"n2_d1\")(F1_G)\n",
    "    D1_G = Multiply()([A5, D1_G])\n",
    "\n",
    "    D2_G = Dense(units = 256, activation = 'linear', name = \"n2_d2\")(D1_G)\n",
    "    D2_G = Multiply()([A6, D2_G])\n",
    "\n",
    "    outputs = Dense(units = 10, activation = 'softmax', name = \"n2_output\")(D2_G)\n",
    "\n",
    "    model = tfk.Model(inputs = inputs, outputs = outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "eps, beta = 0.1, 4\n",
    "\n",
    "class SoftGate(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftGate, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(SoftGate, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        activation = (1 + eps)*K.sigmoid(beta*x)\n",
    "        return activation\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "def getDecoupledLearning():\n",
    "    inputs = Input(shape = (img_wid, img_wid, 3))\n",
    "    #V1\n",
    "    C1 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c1\",\n",
    "                           activation = 'linear')(inputs)\n",
    "    C1_A = Activation('relu')(C1)\n",
    "    C2 = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n1_c2\",\n",
    "                           activation = 'linear')(C1_A)\n",
    "    C2_A = Activation('relu')(C2)\n",
    "    C3 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c3\",\n",
    "                           activation = 'linear')(C2_A)\n",
    "    C3_A = Activation('relu')(C3)\n",
    "    C4 = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n1_c4\",\n",
    "                           activation = 'linear')(C3_A)\n",
    "    C4_A = Activation('relu')(C4)\n",
    "    G1 = GlobalAveragePooling2D()(C4_A)\n",
    "    F1 = Flatten()(G1)\n",
    "    D1 = Dense(units = 256, activation = 'linear', name = \"n1_d1\")(F1)\n",
    "    D1_A = Activation('relu')(D1)\n",
    "    D2 = Dense(units = 256, activation = 'linear', name = \"n1_d2\")(D1_A)\n",
    "\n",
    "    A1 = SoftGate()(C1)\n",
    "    A2 = SoftGate()(C2)\n",
    "    A3 = SoftGate()(C3)\n",
    "    A4 = SoftGate()(C4)\n",
    "    A5 = SoftGate()(D1)\n",
    "    A6 = SoftGate()(D2)\n",
    "    C1_G = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n2_c1\",\n",
    "                           activation = 'linear')(inputs)\n",
    "    C1_G = Multiply()([A1, C1_G])\n",
    "\n",
    "    C2_G = Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', name = \"n2_c2\", \n",
    "                           activation = 'linear')(C1_G)\n",
    "    C2_G = Multiply()([A2, C2_G])\n",
    "\n",
    "    #V2\n",
    "    C3_G = Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', name = \"n2_c3\",\n",
    "                           activation = 'linear')(C2_G)\n",
    "    C3_G = Multiply()([A3, C3_G])\n",
    "\n",
    "    C4_G = Conv2D(filters = 128, kernel_size = (3, 3),  padding = 'same', name = \"n2_c4\",\n",
    "                           activation = 'linear')(C3_G)\n",
    "    C4_G = Multiply()([A4, C4_G])\n",
    "\n",
    "    G1_G = GlobalAveragePooling2D()(C4_G)\n",
    "    F1_G = Flatten()(G1_G)\n",
    "\n",
    "    D1_G = Dense(units = 256, activation = 'linear', name = \"n2_d1\")(F1_G)\n",
    "    D1_G = Multiply()([A5, D1_G])\n",
    "\n",
    "    D2_G = Dense(units = 256, activation = 'linear', name = \"n2_d2\")(D1_G)\n",
    "    D2_G = Multiply()([A6, D2_G])\n",
    "\n",
    "    outputs = Dense(units = 10, activation = 'softmax', name = \"n2_output\")(D2_G)\n",
    "\n",
    "    model = tfk.Model(inputs = inputs, outputs = outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "loss = tfk.losses.categorical_crossentropy\n",
    "opt = tfk.optimizers.Adam\n",
    "batch_size = 32\n",
    "num_exp = 5\n",
    "num_epochs = 80\n",
    "\n",
    "history_relu = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n",
    "history_galu_learned = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n",
    "history_frozen_relu = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n",
    "history_galu = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}\n",
    "history_decoupled_learning = {'acc':[], 'val_acc':[], 'loss': [], 'val_loss': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_i in range(num_exp):\n",
    "    print(\"_____________EXP:{}____________\".format(exp_i+1))\n",
    "\n",
    "    model_relu = getConv4Relu()\n",
    "    model_relu.compile(loss = loss, optimizer = opt(lr), metrics = ['acc'])\n",
    "\n",
    "    filepath=\"weights.best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    history = model_relu.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n",
    "                                batch_size=batch_size, epochs= num_epochs, callbacks = callbacks_list)\n",
    "    history_relu['acc'].append(history.history['acc'])\n",
    "    history_relu['val_acc'].append(history.history['val_acc'])\n",
    "    history_relu['loss'].append(history.history['loss'])\n",
    "    history_relu['val_loss'].append(history.history['val_loss'])\n",
    "    print(\"ReLU: MAX ACC = {}, MAX VAL ACC = {}\".format(np.max(history.history['acc']), \n",
    "                                                        np.max(history.history['val_acc'])))\n",
    "    model_relu.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "    model_galu_learned = getConv4Galu()\n",
    "    layers_relu = getNetwork1Layers(model_relu)[:-1]\n",
    "    layers_galu_learned = getNetwork1Layers(model_galu_learned)\n",
    "\n",
    "    for layer in layers_galu_learned:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model_galu_learned.compile(loss = loss, optimizer = opt(lr), metrics = ['acc'])\n",
    "\n",
    "    for layer1,layer2 in zip(layers_galu_learned, layers_relu): \n",
    "        layer1.set_weights(layer2.get_weights())\n",
    "\n",
    "    history = model_galu_learned.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n",
    "                                     batch_size=batch_size, epochs= num_epochs)\n",
    "    history_galu_learned['acc'].append(history.history['acc'])\n",
    "    history_galu_learned['val_acc'].append(history.history['val_acc'])\n",
    "    history_galu_learned['loss'].append(history.history['loss'])\n",
    "    history_galu_learned['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "    print(\"GaLU Learned: MAX ACC = {}, MAX VAL ACC = {}\".format(np.max(history.history['acc']), \n",
    "                                                        np.max(history.history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ReLU: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n",
    "                                                    np.mean(np.max(history_relu['acc'], axis = 1)), \n",
    "                                                    np.mean(np.max(history_relu['val_acc'], axis = 1)),\n",
    "                                                    np.std(np.max(history_relu['val_acc'], axis = 1))))\n",
    "\n",
    "print(\"GaLU Learned: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n",
    "                                                    np.mean(np.max(history_galu_learned['acc'], axis = 1)), \n",
    "                                                    np.mean(np.max(history_galu_learned['val_acc'], axis = 1)),\n",
    "                                                    np.std(np.max(history_galu_learned['val_acc'], axis = 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"h_cifar10_gconv_adam_relu\", 'wb')\n",
    "pickle.dump(history_relu, file)\n",
    "\n",
    "file = open(\"h_cifar10_gconv_adam_galu_learned\", 'wb')\n",
    "pickle.dump(history_galu_learned, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frozen ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_i in range(num_exp):\n",
    "    print(\"_____________EXP:{}____________\".format(exp_i+1))\n",
    "    model_frozen_relu = getConv4Galu()\n",
    "    layers_frozen_relu_n1 = getNetwork1Layers(model_frozen_relu)\n",
    "    layers_frozen_relu_n2 = getNetwork2Layers(model_frozen_relu)\n",
    "\n",
    "    for layer in layers_frozen_relu_n1:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model_frozen_relu.compile(loss = loss, optimizer = opt(lr), metrics = ['acc'])\n",
    "\n",
    "    for layer1, layer2 in zip(layers_frozen_relu_n1, layers_frozen_relu_n2):\n",
    "        layer1.set_weights(layer2.get_weights())    \n",
    "\n",
    "    history = model_frozen_relu.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n",
    "                                batch_size=batch_size, epochs= num_epochs)\n",
    "    history_frozen_relu['acc'].append(history.history['acc'])\n",
    "    history_frozen_relu['val_acc'].append(history.history['val_acc'])\n",
    "    history_frozen_relu['loss'].append(history.history['loss'])\n",
    "    history_frozen_relu['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "    print(\"Frozen ReLU: MAX ACC = {:.4f}, MAX VAL ACC = {:.4f}\".format(np.max(history.history['acc']), \n",
    "                                                        np.max(history.history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Frozen ReLU: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n",
    "                                                    np.mean(np.max(history_frozen_relu['acc'], axis = 1)), \n",
    "                                                    np.mean(np.max(history_frozen_relu['val_acc'], axis = 1)),\n",
    "                                                    np.std(np.max(history_frozen_relu['val_acc'], axis = 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"h_cifar10_gconv_adam_frozen_relu\", 'wb')\n",
    "pickle.dump(history_frozen_relu, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GaLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_i in range(num_exp):\n",
    "    print(\"_____________EXP:{}____________\".format(exp_i+1))\n",
    "    model_galu = getConv4Galu()\n",
    "    layers_frozen_relu_n1 = getNetwork1Layers(model_galu)\n",
    "\n",
    "    for layer in layers_frozen_relu_n1:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model_galu.compile(loss = loss, optimizer = opt(lr), metrics = ['acc'])\n",
    "\n",
    "    history = model_galu.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n",
    "                                batch_size=batch_size, epochs= 100)\n",
    "    history_galu['acc'].append(history.history['acc'])\n",
    "    history_galu['val_acc'].append(history.history['val_acc'])\n",
    "    history_galu['loss'].append(history.history['loss'])\n",
    "    history_galu['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "    print(\"GaLU: MAX ACC = {:.4f}, MAX VAL ACC = {:.4f}\".format(np.max(history.history['acc']), \n",
    "                                                        np.max(history.history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GaLU: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n",
    "                                                    np.mean(np.max(history_galu['acc'], axis = 1)), \n",
    "                                                    np.mean(np.max(history_galu['val_acc'], axis = 1)),\n",
    "                                                    np.std(np.max(history_galu['val_acc'], axis = 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"h_cifar10_gconv_adam_gap_galu\", 'wb')\n",
    "pickle.dump(history_galu, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decoupled Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_i in range(num_exp):\n",
    "    print(\"_____________EXP:{}____________\".format(exp_i+1))\n",
    "    model_dl = getDecoupledLearning(depth, width)\n",
    "    model_dl.compile(loss = loss, optimizer = opt(lr), metrics = ['acc'])\n",
    "\n",
    "    history = model_dl.fit(x_train, y_train, validation_data = (x_test, y_test), verbose = 0,\n",
    "                             batch_size=batch_size, epochs= num_epochs)\n",
    "    \n",
    "    history_decoupled_learning['acc'].append(history.history['acc'])\n",
    "    history_decoupled_learning['val_acc'].append(history.history['val_acc'])\n",
    "    history_decoupled_learning['loss'].append(history.history['loss'])\n",
    "    history_decoupled_learning['val_loss'].append(history.history['val_loss'])\n",
    "\n",
    "    print(\"Decoupled Learning: MAX ACC = {:.4f}, MAX VAL ACC = {:.4f}\".format(\n",
    "                                                        np.max(history.history['acc']), \n",
    "                                                        np.max(history.history['val_acc'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decoupled Learning: max_acc = {:.4f}, mean_max_val_acc = {:.4f}, std_max_val_acc = {:.4f}\".format(\n",
    "                                            np.mean(np.max(history_decoupled_learning['acc'], axis = 1)), \n",
    "                                            np.mean(np.max(history_decoupled_learning['val_acc'], axis = 1)),\n",
    "                                            np.std(np.max(history_decoupled_learning['val_acc'], axis = 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('h_cifar10_gconv_adam_gap_decoupled_learning', 'wb')\n",
    "pickle.dump(history_decoupled_learning, file)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 1
}
